{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e181b761",
   "metadata": {},
   "source": [
    "-- Contents -- <br>\n",
    "1. Simple and fast text extraction\n",
    "2. Advanced parsing\n",
    "3. Sitemap extraction\n",
    "4. Indexing parsed webpage data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938c4ec",
   "metadata": {},
   "source": [
    "# 1. Simple and fast text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b198c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbe7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url1 = \"https://hongkongfp.com/2025/06/29/explainer-how-national-security-permeates-hong-kong-bureaucracy-5-years-after-law-enacted/\"\n",
    "page_url2 = \"https://news.rthk.hk/rthk/en/component/k2/1811238-20250630.htm\"\n",
    "\n",
    "loader = WebBaseLoader(web_paths=[page_url1,\n",
    "                                  page_url2])\n",
    "docs = []\n",
    "\n",
    "async for doc in loader.alazy_load():\n",
    "    docs.append(doc)\n",
    "\n",
    "## -- ASYNC -- ##\n",
    "# The async for statement allows convenient iteration over asynchronous iterables.\n",
    "# Asynchronous programming in Python is the process of writing concurrent code that runs asynchronously – \n",
    "# i.e. doesn't take place in real-time.\n",
    "# It allows an app instance to execute multiple tasks at the same time, or in parallel.\n",
    "\n",
    "assert len(docs) == 2\n",
    "doc1 = docs[0]\n",
    "doc2 = docs[1]\n",
    "\n",
    "## -- ASSERT -- ##\n",
    "# In Python, the assert statement is used for debugging and testing purposes. \n",
    "# It allows you to create assertions, which are essentially sanity checks that verify if a certain condition is true \n",
    "# during the execution of your program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "848dee39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://hongkongfp.com/2025/06/29/explainer-how-national-security-permeates-hong-kong-bureaucracy-5-years-after-law-enacted/', 'title': 'How nat. sec permeates HK bureaucracy, 5 years after law enacted', 'description': \"HK gov't departments and statutory bodies for different sectors, including education, labour, social welfare, arts and culture, and the environment, have adopted nat. sec clauses in their guidelines and conditions since Beijing imposed the law in June 2020.\", 'language': 'en-GB'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint as p \n",
    "print(f\"{doc1.metadata}\\n\")\n",
    "#p.pprint(doc1.page_content[:500].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20093ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_loader = WebBaseLoader(\n",
    "    web_paths=[page_url1],\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(class_=\"entry-title entry-title--with-subtitle\"\n",
    "        ),\n",
    "    },\n",
    "    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},\n",
    ")\n",
    "\n",
    "title = []\n",
    "async for doc in title_loader.alazy_load():\n",
    "    title.append(doc)\n",
    "\n",
    "assert len(title) == 1\n",
    "title = title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df0aacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=[page_url1],\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(# class_=\"entry-title entry-title--with-subtitle\")\n",
    "                                       # class_= \"newspack-post-subtitle\"\n",
    "                                       class_ = \"main-content\"\n",
    "\n",
    "        ),\n",
    "    },\n",
    "    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},\n",
    ")\n",
    "\n",
    "docs = []\n",
    "async for doc in loader.alazy_load():\n",
    "    docs.append(doc)\n",
    "\n",
    "assert len(docs) == 1\n",
    "doc = docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "625a4ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://hongkongfp.com/2025/06/29/explainer-how-national-security-permeates-hong-kong-bureaucracy-5-years-after-law-enacted/', 'title': 'x'}\n",
      "\n",
      "('Explainer: How national security permeates Hong Kong bureaucracy, 5 years '\n",
      " 'after law enacted')\n",
      "('Five years since the Beijing-imposed legislation came into effect in Hong '\n",
      " 'Kong, national security terms have become increasingly common in official '\n",
      " 'guidelines, permit applications, and licences issued by government '\n",
      " 'departments and semi-official bodies. | Most recently, the Food and '\n",
      " 'Environmental Hygiene Department (FEHD) notified businesses of new national '\n",
      " 'security clauses under the Public Health and Municipal Services Ordinance. | '\n",
      " 'China’s national flags fill the streets in Hong Kong ahead of July 1, 2025, '\n",
      " 'the 28th anniversary of Hong Kong’s handover to China. Photo: Kyle Lam/HKFP. '\n",
      " '| The new rule is the latest addition to similar provisions in official '\n",
      " 'guidelines. Government departments and statutory bodies across different '\n",
      " 'sectors, including education, labour, social welfare, arts and culture, and '\n",
      " 'the environment, have added clauses relating to national security to their '\n",
      " 'terms and conditions. | John Burns, honorary professor in the Department of '\n",
      " 'Politics and Public Administration at the University of Hong Kong, believes '\n",
      " 'that the increasing prevalence of national security provisions in government '\n",
      " 'guidelines and conditions reflects an intention to align the city’s '\n",
      " 'political culture with that of mainland China. | “Hong Kong is learning a '\n",
      " 'new way of behaving,” Burns said, adding that he believed such measures '\n",
      " 'would primarily function, in effect, as a deterrent mechanism. “What they’re '\n",
      " 'encouraging everyone to do is self-censor, to behave.” | As Hong Kong marks '\n",
      " 'the fifth anniversary of the enactment of the national security law on '\n",
      " 'Monday, HKFP looks at how the government has made national security '\n",
      " 'near-ubiquitous in the years since the law was enacted at the end of June '\n",
      " '2020. | From restaurants to the environment | First reported in early June | '\n",
      " ', the FEHD’s new guideline affects restaurants, entertainment premises like '\n",
      " 'cinemas, gaming centres, and saunas, as well as funeral parlours. | In '\n",
      " 'letters sent to businesses, the department said that it could revoke '\n",
      " 'business licences if operators – including license holders, directors, '\n",
      " 'management, employees, agents, and subcontractors – engage in “offending '\n",
      " 'conduct” against national security or the public interest. | A Japanese '\n",
      " 'restaurant in Shek Tong Tsui. Photo: Kyle Lam/HKFP. | The move has raised '\n",
      " 'suspicions as to whether it targets “yellow shops” – businesses sympathetic '\n",
      " 'to Hong Kong’s democracy\\xa0movement. | In addition, Secretary for '\n",
      " 'Environment and Ecology Tse Chin-wan pledged earlier this month to tighten '\n",
      " 'scrutiny of applicants and recipients of the government’s | Environment and '\n",
      " 'Conservation Fund (ECF) | , saying public resources must not fall into the '\n",
      " 'hands of “non-patriots.” | Applications for community waste reduction '\n",
      " 'projects supported by the ECF come with an | agreement | for NGOs to '\n",
      " 'safeguard national security. | NGOs | applying for community waste reduction '\n",
      " 'projects that receive ECF support are required to sign a declaration '\n",
      " 'pledging compliance with the security law and all local laws. Authorities '\n",
      " 'also review the background and past work of applicant organisations to '\n",
      " 'verify that they are “patriotic.” | Previously, in February 2023, 11 green '\n",
      " 'groups, including ECF beneficiary Greensense, were | named | in a Wen Wei Po '\n",
      " 'report, accusing them of inciting hatred against the government at a '\n",
      " 'fundraising event. | Education | Theatre troupe Fire Makes Us Human was left '\n",
      " 'without a venue | for two plays in February last year after a school '\n",
      " 'cancelled its venue booking, citing an instruction from the Education Bureau '\n",
      " '(EDB) under national security guidelines. | The Hong Kong Institute of '\n",
      " 'Contemporary Culture Lee Shau Kee School of Creativity (HKICC) told HKFP '\n",
      " 'that it had complied with the EDB’s request after the bureau received '\n",
      " 'complaints about remarks on “controversial” issues made by the troupe’s '\n",
      " 'founder, Alex Tong. | Under national security guidelines, schools should '\n",
      " '“prevent inappropriate use of school premises,” including situations where '\n",
      " 'facilities are rented out to external organisations and when external '\n",
      " 'individuals are invited to participate in school events. | A man poses for a '\n",
      " 'photo on the 10th National Security Education Day, on April 15, 2025. Photo: '\n",
      " 'Kyle Lam/HKFP. | The responsibility falls on school management to forbid any '\n",
      " 'person from conducting activities involving “political propaganda” on '\n",
      " 'campus. | The 34-page document lists specific national security measures in '\n",
      " 'schools, including a requirement for all newly appointed teachers at public '\n",
      " 'schools, Direct Subsidy Scheme schools, and kindergartens joining the '\n",
      " 'Kindergarten Education Scheme to pass the Basic Law and National Security '\n",
      " 'Law Test. | Contracts and quotations issued by government-subsidised schools '\n",
      " 'must also include clauses relating to safeguarding national security. | '\n",
      " 'Under a separate document, the School Administration Guide, school '\n",
      " 'librarians are required to ensure that the school library does not have any '\n",
      " 'elements that endanger national security. Public libraries are similarly '\n",
      " 'required to scrut')\n"
     ]
    }
   ],
   "source": [
    "print(f\"{doc.metadata}\\n\")\n",
    "p.pprint(title.page_content)\n",
    "p.pprint(doc.page_content[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109a6b0",
   "metadata": {},
   "source": [
    "# 2. Advanced parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b39e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: NumExpr defaulting to 14 threads.\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "page_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"\n",
    "loader = UnstructuredLoader(web_url=page_url)\n",
    "\n",
    "docs = []\n",
    "async for doc in loader.alazy_load():\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c25b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open In Colab\n",
      "Open on GitHub\n",
      "How to add memory to chatbots\n",
      "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n",
      "Simply stuffing previous messages into a chat model prompt.\n",
      "The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n",
      "More complex modifications like synthesizing summaries for long running conversations.\n",
      "We'll go into more detail on a few techniques below!\n",
      "note\n",
      "This how-to guide previously built a chatbot using RunnableWithMessageHistory. You can access this version of the guide in the v0.2 docs.\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a060f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Open In Colab\n",
      "Image: Open on GitHub\n",
      "Title: How to add memory to chatbots\n",
      "NarrativeText: A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n",
      "ListItem: Simply stuffing previous messages into a chat model prompt.\n",
      "ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n",
      "ListItem: More complex modifications like synthesizing summaries for long running conversations.\n",
      "NarrativeText: We'll go into more detail on a few techniques below!\n",
      "UncategorizedText: note\n",
      "NarrativeText: This how-to guide previously built a chatbot using RunnableWithMessageHistory. You can access this version of the guide in the v0.2 docs.\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(f'{doc.metadata[\"category\"]}: {doc.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d0409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "async def _get_setup_docs_from_url(url: str) -> List[Document]:\n",
    "    loader = UnstructuredLoader(web_url=url)\n",
    "\n",
    "    setup_docs = []\n",
    "    parent_id = -1\n",
    "\n",
    "    async for doc in loader.alazy_load():\n",
    "        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):\n",
    "            parent_id = doc.metadata[\"element_id\"]\n",
    "        if doc.metadata.get(\"parent_id\") == parent_id:\n",
    "            setup_docs.append(doc)\n",
    "\n",
    "    return setup_docs\n",
    "\n",
    "\n",
    "page_urls = [\n",
    "    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",\n",
    "    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",\n",
    "]\n",
    "\n",
    "setup_docs = []\n",
    "for url in page_urls:\n",
    "    page_setup_docs = await _get_setup_docs_from_url(url)\n",
    "    setup_docs.extend(page_setup_docs)\n",
    "\n",
    "## -- AWAIT -- ##\n",
    "\n",
    "# Usage within async functions:\n",
    "# The await keyword can only be used inside functions defined with the async def syntax, which designates them as coroutines.\n",
    "# Pausing execution:\n",
    "# When await is encountered, the current coroutine's execution is temporarily suspended, and control is yielded back to the event loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753bcdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '045f743be3cd8ffd40e1856dcbe29eca', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'a9382c193b18dd10455d6510ff3651dd'}, page_content=\"You'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\"),\n",
       " Document(metadata={'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '045f743be3cd8ffd40e1856dcbe29eca', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '1c452f9bedb9da8c85946b7886292f50'}, page_content='%pip install --upgrade --quiet langchain langchain-openai langgraph\\n\\nimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")'),\n",
       " Document(metadata={'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '045f743be3cd8ffd40e1856dcbe29eca', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'UncategorizedText', 'element_id': '1ad77afba3f6f04a61ed7e6b6630abf0'}, page_content='OpenAI API Key: ········'),\n",
       " Document(metadata={'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '045f743be3cd8ffd40e1856dcbe29eca', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'c70ecf3e59c7ee729fdb50f9ebc0afd5'}, page_content=\"Let's also set up a chat model that we'll use for the below examples.\"),\n",
       " Document(metadata={'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '045f743be3cd8ffd40e1856dcbe29eca', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'UncategorizedText', 'element_id': '97bc29a443a70d8bf960861fc035e98a'}, page_content='from langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")'),\n",
       " Document(metadata={'emphasized_text_contents': ['API Reference:'], 'emphasized_text_tags': ['b'], 'link_texts': ['ChatOpenAI'], 'link_urls': ['https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '045f743be3cd8ffd40e1856dcbe29eca', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'UncategorizedText', 'element_id': '3e180643ab1d403a53735f0b201cad06'}, page_content='API Reference:ChatOpenAI'),\n",
       " Document(metadata={'link_texts': ['tool calling agent', 'Tavily'], 'link_urls': ['https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#tool-calling-agent', '/docs/integrations/tools/tavily_search/'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'b742b82857d0914360bfa7b7e9afb984', 'url': 'https://python.langchain.com/docs/how_to/chatbots_tools/', 'category': 'NarrativeText', 'element_id': '5fd4863233f5f38c3957edfc8b80741e'}, page_content=\"For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\"),\n",
       " Document(metadata={'link_texts': ['sign up for an account'], 'link_urls': ['https://tavily.com/'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'b742b82857d0914360bfa7b7e9afb984', 'url': 'https://python.langchain.com/docs/how_to/chatbots_tools/', 'category': 'NarrativeText', 'element_id': 'c80534a49e467098d47763089ae4350f'}, page_content=\"You'll need to sign up for an account on the Tavily website, and install the following packages:\"),\n",
       " Document(metadata={'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'b742b82857d0914360bfa7b7e9afb984', 'url': 'https://python.langchain.com/docs/how_to/chatbots_tools/', 'category': 'NarrativeText', 'element_id': 'ada1f914d0b025dd748293716ce86be7'}, page_content='%pip install --upgrade --quiet langchain-openai tavily-python langgraph\\n\\nimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nif not os.environ.get(\"TAVILY_API_KEY\"):\\n    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API Key:\")'),\n",
       " Document(metadata={'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'b742b82857d0914360bfa7b7e9afb984', 'url': 'https://python.langchain.com/docs/how_to/chatbots_tools/', 'category': 'NarrativeText', 'element_id': 'abdcc2136c30f6d7b294a67966b22cb3'}, page_content='You will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fde83dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://python.langchain.com/docs/how_to/chatbots_memory/': 'You\\'ll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\\n%pip install --upgrade --quiet langchain langchain-openai langgraph\\n\\nimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nOpenAI API Key: ········\\nLet\\'s also set up a chat model that we\\'ll use for the below examples.\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nAPI Reference:ChatOpenAI\\n',\n",
       " 'https://python.langchain.com/docs/how_to/chatbots_tools/': 'For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\nYou\\'ll need to sign up for an account on the Tavily website, and install the following packages:\\n%pip install --upgrade --quiet langchain-openai tavily-python langgraph\\n\\nimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nif not os.environ.get(\"TAVILY_API_KEY\"):\\n    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API Key:\")\\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "setup_text = defaultdict(str)\n",
    "\n",
    "for doc in setup_docs:\n",
    "    url = doc.metadata[\"url\"]\n",
    "    setup_text[url] += f\"{doc.page_content}\\n\"\n",
    "\n",
    "dict(setup_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994b533",
   "metadata": {},
   "source": [
    "# 3. Sitemap extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771cfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64912298",
   "metadata": {},
   "source": [
    "# 4. Indexing parsed webpage data\n",
    "\n",
    "- Loading HuggingFace embedding (Rmk. This part doesn't work with API access neither with HF Hub connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80497518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Use pytorch device_name: mps\n",
      "INFO: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") #22M parameters: 3minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67f2e31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b12167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.03833858668804169, 0.12346469610929489, -0.02864297293126583]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small test:\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f9d2953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page https://python.langchain.com/docs/how_to/chatbots_memory/: Let's also set up a chat model that we'll use for the below examples.\n",
      "\n",
      "Page https://python.langchain.com/docs/how_to/chatbots_memory/: API Reference:ChatOpenAI\n",
      "\n",
      "Page https://python.langchain.com/docs/how_to/chatbots_memory/: from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
      "\n",
      "Page https://python.langchain.com/docs/how_to/chatbots_memory/: OpenAI API Key: ········\n",
      "\n",
      "Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(setup_docs, embeddings)\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(\"chat model\", k=5)\n",
    "for doc in retrieved_docs:\n",
    "    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
